# -*- coding: utf-8 -*-
"""word2vec_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z0XhsAYLi-SI97Vh0HZFtaDKZLFZuLBi
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from collections import Counter
import random
import re

import nltk
nltk.download('reuters')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import reuters
import re

categories = []
fileCount = []
for i in reuters.categories():
  fileCount.append(len(reuters.fileids(i)))
  categories.append(i)
print(fileCount)
print(categories)

fileids = reuters.fileids()
print(fileids)

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from collections import Counter

#nltk.download('stopwords')
stopword = set(stopwords.words('english'))
print(stopword)

from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
stemmer= PorterStemmer()
lemmatizer=WordNetLemmatizer()
import string
tokenized_corpus = []
for i in fileids:
  a = reuters.raw(i)
  a = a.lower()
  a = re.sub("^\d+\s|\s\d+\s|\s\d+$", " ", a)
  a = re.sub(r'\d+', '', a)
  a = a.translate(str.maketrans("","", string.punctuation))
  #a = a.translate(None, string.punctuation)
  words = word_tokenize(a)
  a = []
  b = []
  #for word in words:
    #a.append(stemmer.stem(word))'''
  #for word in words:
    #b.append(lemmatizer.lemmatize(word))
  result = [w for w in words if not w in stopword]
  tokenized_corpus.append(result)

print(tokenized_corpus[0][0])

vocabulary = []
for sentence in tokenized_corpus:
    for token in sentence:
        if token not in vocabulary:
          if token.isalpha():
            vocabulary.append(token)

sorted(vocabulary)
#print(vocabulary)

word_to_index = {word: index for (index, word) in enumerate(vocabulary)}
index_to_word = {index: word for (index, word) in enumerate(vocabulary)}
cnt = Counter()
for l in tokenized_corpus:
  for word in l:
    cnt[word] += 1
indices = {word:cnt[word] for word in vocabulary}

threshold = 1e-5
total_count = len(tokenized_corpus)
freqs = {word: count/total_count for word, count in indices.items()}
p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in indices}
train_words = []
for sentence in tokenized_corpus:
    for word in sentence:
      if random.random() < (1 - p_drop[word]):
        train_words.append(word)
      
#train_words = [word for word in tokenized_corpus if random.random() < (1 - p_drop[word])]

window_size = 2
index_pairs = []
for sentence in tokenized_corpus:
    indices = [word_to_index[word] for word in sentence]
    for center_word_pos in range(len(indices)):
        for w in range(-window_size, window_size):
            context_word_pos = center_word_pos + w
            if context_word_pos < 0 or context_word_pos >= len(indices) or center_word_pos == context_word_pos:
                continue
            context_word_index = indices[context_word_pos]
            index_pairs.append((indices[center_word_pos], context_word_index))

#index_pairs = np.array(index_pairs)

index_pairs = list(set(index_pairs))
#print(len(index_pairs))
#for i in range(100):
#  print(index_pairs[i])

def get_batches(words, batch_size):
    n_batches = len(words)//batch_size
    words = np.array(words[:n_batches*batch_size])
    _cursor = 0
    while True:
        batch = words[_cursor:_cursor+batch_size]
        _cursor = (_cursor + batch_size) % len(words)
        yield batch[:, 0].reshape(-1, 1), batch[:, 1].reshape(-1, 1)

embedding_dim = 300
n_neg_samples = 10
inputs = tf.placeholder(tf.int64, [None, 1], name='inputs')
labels = tf.placeholder(tf.int64, [None, 1], name='labels')
neg_samples, _, _ = tf.random.uniform_candidate_sampler(labels, 1, n_neg_samples, True, len(vocabulary))
context_weights = tf.get_variable('context_embeddings_1', shape=(len(vocabulary), embedding_dim))
negative_weights = tf.get_variable('negative_embeddings_1', shape=(len(vocabulary), embedding_dim))
center_embeddings = tf.reshape(tf.nn.embedding_lookup(context_weights, inputs), (-1, embedding_dim))
context_embeddings = tf.reshape(tf.nn.embedding_lookup(context_weights, labels), (-1, embedding_dim))
negative_embeddings = tf.negative(tf.nn.embedding_lookup(negative_weights, neg_samples))
pos_loss = tf.reduce_mean(tf.log_sigmoid(tf.reduce_sum(tf.multiply(center_embeddings, context_embeddings), axis=1)), axis=0)
neg_loss = tf.reduce_mean(tf.reduce_sum(tf.log_sigmoid(tf.matmul(center_embeddings, tf.transpose(negative_embeddings))), axis=1), axis=0)
loss = tf.negative(pos_loss + neg_loss)
opt = tf.train.AdamOptimizer(0.01).minimize(loss)

#tf.reset_default_graph()
sess = tf.Session()
uninitialized_vars = []
for var in tf.global_variables():
    try:
        sess.run(var)
    except tf.errors.FailedPreconditionError:
        uninitialized_vars.append(var)

init_new_vars_op = tf.initialize_variables(uninitialized_vars)
sess.run([init_new_vars_op])

epochs = 10
itr = 1
batch_size = 400
data_len = len(index_pairs)//batch_size
for i in range(1,epochs+1):
    for x, y in get_batches(index_pairs, batch_size):
        _, step_loss = sess.run([opt, loss], feed_dict={inputs:x,labels:y})
        if itr%data_len == 0:
            print(step_loss)
        itr +=1

saver = tf.train.Saver()
saver.save(sess, 'word2vec.ckpt')

g = tf.Graph()
sess = tf.Session(graph=g)
saver = tf.train.import_meta_graph('word2vec.ckpt.meta', graph=g)
saver.restore(sess, 'word2vec.ckpt')
graph = sess.graph
embedding_matrix = graph.get_tensor_by_name('context_embeddings_2:0')
embedding_matrix = np.array(sess.run([embedding_matrix])[0])
embedding_matrix = embedding_matrix / np.linalg.norm(embedding_matrix, axis=1).reshape(-1, 1)

#np.abs(np.dot(embedding_matrix[0], embedding_matrix[1]))